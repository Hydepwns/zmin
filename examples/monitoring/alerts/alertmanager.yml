# Alertmanager configuration for zmin alerts
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

# Templates for alert messages
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  group_by: ['alertname', 'severity', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Critical alerts go to pagerduty
    - match:
        severity: critical
      receiver: pagerduty
      continue: true
    
    # Performance alerts go to slack
    - match:
        alertname: LowThroughput
      receiver: slack-performance
    
    # GPU alerts
    - match_re:
        alertname: ^GPU.*
      receiver: slack-gpu
    
    # Everything else goes to default
    - match:
        severity: warning
      receiver: slack-warnings

# Receivers configuration
receivers:
  - name: 'default'
    email_configs:
      - to: 'zmin-alerts@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'password'
        headers:
          Subject: 'Zmin Alert: {{ .GroupLabels.alertname }}'

  - name: 'slack-performance'
    slack_configs:
      - channel: '#zmin-performance'
        title: 'Performance Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'

  - name: 'slack-gpu'
    slack_configs:
      - channel: '#zmin-gpu'
        title: 'GPU Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#zmin-alerts'
        title: 'Zmin Warning'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
        send_resolved: false

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

# Inhibition rules
inhibit_rules:
  # Inhibit warning alerts if critical alerts are firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']
  
  # Inhibit low throughput warnings if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'LowThroughput'
    equal: ['service']